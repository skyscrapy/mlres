\documentclass[11pt]{article}

\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{url}
\usepackage{wrapfig}
\usepackage{hyperref} 
\usepackage{color}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{enumerate}
\usepackage{amsmath,bm}
\oddsidemargin 0mm
\evensidemargin 5mm
\topmargin -20mm
\textheight 240mm
\textwidth 160mm

\newcommand{\alphai}{\alpha_i}
\newcommand{\va}{{\bf \alpha}}
\newcommand{\vw}{{\bf w}}
\newcommand{\vx}{{\bf x}}
\newcommand{\vxi}{{\bf x}_i}
\newcommand{\vxij}{{\bf x}_{ij}}
\newcommand{\vt}{{\bf t}}
\newcommand{\X}{{\mathbf X}}
\newcommand{\Y}{{\bf Y}}
\newcommand{\yh}{\hat{y}}
\newcommand{\yi}{y_i}
\newcommand{\pder}[2][]{\frac{\partial#1}{\partial#2}}

\def\P{\mathbb{P}}
\def\E{\mathbb{E}}
\def\R{\mathbb{R}}
\newcommand{\msigma}{{\bf \Sigma}}
\newcommand{\vmu}{{\bf \mu}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\el}{\mathcal{L}}
\newcommand{\code}[1]{{\footnotesize \tt #1}}
\renewcommand{\baselinestretch}{1.2}
\pagestyle{myheadings} 
\markboth{Homework 2}{Spring 2016 CS 4269/6362 Machine Learning: Homework 3}


\title{CS 4269/6362 Machine Learning: Homework 3\\Non-Linear Supervised Learning\\
\Large{Due: 2/11/2016}\\
100 Points Total \hspace{1cm} Version 1.0}

\author{}
\date{}

\begin{document}
\large
\maketitle
\thispagestyle{headings}

\vspace{-.5in}
\section{Programming (CS 4269: 100 points, CS 6362: 50 points)}
In this assignment you will write a Kernel
logistic regression classifier. This classifier needs only support
binary prediction.
The \code{algorithm} option for this classifier should be \code{kernel\_logistic\_regression}.

\subsection{Logistic Regression}

\subsection{Review: Logistic Regression}

The logistic regression model is used to model binary classification data. Logistic regression is a special case of generalized linear regression where the labels $Y$ are modeled as a linear combination of the data $X$, but in a transformed space specified by $g$, sometimes called the ``link function":
\begin{equation}
P(y=1 | \vx, \vw) = g(\vw \cdot \vx)
\end{equation}
\\
This ``link function" allows you to model inherently non-linear data with a linear model. In the case of logistic regression, the link function is the logistic function:
\begin{equation}
g(z) = \frac{1}{1 + e^{-z}}
\end{equation}

\subsection{Kernel Logistic Regression}

The above model relies on a linear function of weight parameters and instances: $\vw \cdot \vx$. However, some data can be highly non-linear, which is impossible to learn using a linear classifier. In class, we learned about the kernel trick, which uses the dual of a linear classifier for kernel learning. Kernels project data into high dimensional spaces in which a linear separator may exist. The decision boundary learned by the linear classifier is linear in the high dimensional space but non-linear in the original feature space.

A kernel is a function which maps the input data into a new space using a basis function and computes the inner product between two basis functions. A kernel function is defined as:
\[
K(x, x') = (\phi(x) \cdot \phi(x')^T)
\]

In this assignment you will implement a dual version of the logistic regression model so that you can learn with a kernel. 
In class, we derived the updates for the dual SVM, and we learned that the linear function of weights $\vw$ and instances $\vx$ can be rewritten:
\[
\vw \cdot \vx = \sum_{i=1}^N \alpha_i \yi K(\vxi, \vx)
\]
This dual formulation also holds in logistic regression. Under the kernel logistic regression model, we have:

\begin{align}
P(y=1 | \vx, \va) &  =   g(\sum_{i=1}^N \alpha_i K(\vxi, \vx)) \\
P(y=0 | \vx, \va) & =   1 - g(\sum_{i=1}^N \alpha_i K(\vxi, \vx))
\end{align}

Note that the $\yi$ terms from the SVM formula are not needed here.
Instead of learning $\vw$, you will learn $\alphai$ for each example $\vxi$. The next subsection describes the learning procedure you will use.

\subsubsection{Training with Gradient Ascent}

In this assignment, we will solve for the parameters $\va$ in our logistic regression model using gradient ascent to find the maximum likelihood estimate.

Gradient ascent is an optimization technique that is both very simple and powerful. This optimization algorithm works by taking the gradient of the objective function and taking steps in directions where the gradient is positive, which increases the objective function.\footnote{In machine learning, we can either maximize or minimize an objective. In this assignment, we are {\em maximizing} an objective function (likelihood) as part of maximum likelihood estimation. One might also want to {\em minimize} an objective function that we think of as a {\em loss} or {\em error}. In this case, the algorithm is called gradient {\bf descent}. You can trivially convert a maximization problem into a minimization problem by negating the objective function, so the terms gradient descent and gradient ascent are sometimes used interchangeably.}
Under certain conditions, gradient ascent is guaranteed to asymptotically converge to a local maximum of the objective function (or a global maximum if the objective is convex). This strategy is often referred to as ``hill climbing.''

Given a likelihood function $\ell(Y,X,\va)$, we need to compute the gradient $\nabla \ell(Y,X,\va)$. The partial derivative with respect to the parameter $\alpha_k$ is:

\begin{equation}
\pder[\ell]{\alpha_k} = \sum_{i=1}^{N} y_i g\left(-\textstyle \sum_{j=1}^{N} \alpha_j K(x_j, x_i)\right) \left(K(x_i, x_k)\right) 
+ (1 - y_i) g\left(\textstyle \sum_{j=1}^{N} \alpha_j K(x_j, x_i)\right) \left(-K(x_i, x_k)\right) 
\end{equation}

\noindent You can see how this is derived in the appendix of this handout.

In gradient descent, the update rule is $\va'_j = \va_j + \eta \pder{\va_j}$ for all values of $j$ (one per training instance), where $\eta$ is called the {\em learning rate}. Choosing $\eta$ intelligently is a non-trivial task, and there are many ways to choose it in the literature. In this assignment, we will use a constant scalar $\eta$. See section \ref{sec:learning_rate} for more details. 

This update is repeated for a given number of training iterations (see \ref{sec:iterations}).

\subsubsection{Making Predictions}

Given an instance $\vxi$, you can predict a label $\yi$ as the $y$ value with higher probability under the model. You should choose a prediction $\hat{\yi}$ according to:

\begin{equation}
\hat{\yi} = {\arg\max}_{y} P(y_i = y | \vxi, \va)
\end{equation}

In other words, if $g(\sum_{j=1}^N \alpha_j  K(\vx_j, \vx_i)) \ge 0.5$, you should predict $\yi$ to be 1, otherwise you predict $\yi$ to be 0.

\subsubsection{Kernel Functions}

You will implement three kernel functions for use with kernel logistic regression. Each version of the kernel logistic regression algorithm will be its own class, but it is highly recommended that most of the code be in a parent class: \code{KernelLogisticRegression} to avoid code duplication. Each subclass will overload and implement the necessary functions for their specific kernels.

\begin{enumerate}
\item Linear Kernel: $K(x, x') = (x \cdot x')$

Implement this algorithm as the \code{LinearKernelLogisticRegression}
class. 
%The command line argument for the \code{algorithm} parameter should be \code{logistic\_regression\_linear\_kernel}.

\item Polynomial Kernel: $K(x, x') = (1 + (x \cdot x'))^d$

Implement this algorithm as the
\code{PolynomialKernelLogisticRegression} class. 
%The command line argument for the \code{algorithm} parameter should be
%\code{logistic\_regression\_polynomial\_kernel}. 
The parameter $d$ is explained in section \ref{sec:kernel_parameters}.

\item Gaussian Kernel: $K(x, x') = \exp(-|| x-x'||^2 / 2 \sigma^2)$

Implement this algorithm as the
\code{GaussianKernelLogisticRegression} class. 
%The command line argument for the \code{algorithm} parameter should be
%\code{logistic\_regression\_gaussian\_kernel}. 
The parameter $\sigma$ is explained in section \ref{sec:kernel_parameters}.

\end{enumerate}

{\bf Note that the naive implementation may compute $K(x_i,x_j)$ many
  times during training. To improve training efficiency you should
  store (cache) computed values of $K(x_i,x_j)$ in a matrix,
  $G_{ij}=K(x_i, x_j)$. $G$ is called a Gram Matrix. If you do not do
  this, your code will run much slower than it should, and we may
  simply assume that it is broken (which may cause you to fail the
  programming part of the assignment).}

You can improve efficiency further by caching 
\[
\sum_{j=1}^{N} \alpha_j K(x_j, x_i)
\] 
for each feature vector $i$ in the data.
We recommend this as well.


\subsection{Implementation Details}

\subsubsection{Kernel}
\label{sec:kernel}

The choice of kernel will be controlled by a command line option.
You need to add this option by adding the following code block to the \code{createCommandLineOptions} method of \code{Learn}.

\begin{footnotesize}
\begin{verbatim}
registerOption("kernel", "String", true, "The kernel for
Kernel Logistic regression [linear_kernel, polynomial_kernel, gaussian_kernel].");
\end{verbatim}
\end{footnotesize}
There are exactly three allowed values for this option:
``linear\_kernel'', ``polynomial\_kernel'', and ``gaussian\_kernel'',
with ``linear\_kernel'' being default when no parameter is supplied.

You can then read the value from the command line by adding the following to the main method of \code{Learn}:
\begin{footnotesize}
\begin{verbatim}
String kernel = "linear_kernel";
if (CommandLineUtilities.hasArg("kernel"))
   kernel = CommandLineUtilities.getOptionValue("kernel");
\end{verbatim}
\end{footnotesize}

\subsubsection{Kernel Parameters}
\label{sec:kernel_parameters}

The parameters $d$ for the polynomial kernel and $\sigma$ for the Gaussian kernel are set using command line options.

For the polynomial kernel parameter $d$, add this command line option by adding the following code block to the \code{createCommandLineOptions} method of \code{Learn}.

\begin{footnotesize}
\begin{verbatim}
registerOption("polynomial_kernel_exponent", "double", true, "The exponent of the polynomial kernel.");
\end{verbatim}
\end{footnotesize}

You can then read the value from the command line by adding the following to the main method of \code{Learn}:
\begin{footnotesize}
\begin{verbatim}
double polynomial_kernel_exponent = 2;
if (CommandLineUtilities.hasArg("polynomial_kernel_exponent"))
   polynomial_kernel_exponent = CommandLineUtilities.getOptionValueAsFloat("polynomial_kernel_exponent");
\end{verbatim}
\end{footnotesize}

For the Gaussian kernel parameter $\sigma$, add this command line option by adding the following code block to the \code{createCommandLineOptions} method of \code{Learn}.

\begin{footnotesize}
\begin{verbatim}
registerOption("gaussian_kernel_sigma", "double", true, "The sigma of the Gaussian kernel.");
\end{verbatim}
\end{footnotesize}

You can then read the value from the command line by adding the following to the main method of \code{Learn}:
\begin{footnotesize}
\begin{verbatim}
double gaussian_kernel_sigma = 1;
if (CommandLineUtilities.hasArg("gaussian_kernel_sigma"))
   gaussian_kernel_sigma = CommandLineUtilities.getOptionValueAsFloat("gaussian_kernel_sigma");
\end{verbatim}
\end{footnotesize}

The default value for $d$ should be $2$ and the default value for $\sigma$ should be $1$.





\subsubsection{Learning Rate}
\label{sec:learning_rate}
Your default value for $\eta$ should be $0.01$. You \emph{must} add a command line argument to allow this value to be adjusted via the command line. 

Add this command line option by adding the following code to the \code{createCommandLineOptions} method of \code{Learn}.
\begin{footnotesize}
\begin{verbatim}
registerOption("gradient_ascent_learning_rate", "double", true, "The learning rate for logistic regression.");
\end{verbatim}
\end{footnotesize}

Be sure to add the option name exactly as it appears above. 
You can then read the value from the command line by adding the following to the main method of \code{Learn}:
\begin{footnotesize}
\begin{verbatim}
double gradient_ascent_learning_rate = 0.01;
if (CommandLineUtilities.hasArg("gradient_ascent_learning_rate"))
    gradient_ascent_learning_rate = 
        CommandLineUtilities.getOptionValueAsFloat("gradient_ascent_learning_rate");
\end{verbatim}
\end{footnotesize}




\subsubsection{Number of Training Iterations}
\label{sec:iterations}

We will define the number of times each algorithm iterates over all of the data by the parameter \code{gradient\_ascent\_training\_iterations}. You \emph{must} define a new command line option for this parameter. Use a default value of $5$ for this parameter.

You can add this option by adding the following code to the \code{createCommandLineOptions} method of \code{Learn}.
\begin{footnotesize}
\begin{verbatim}
registerOption("gradient_ascent_training_iterations", "int", true, "The number of training iterations.");
\end{verbatim}
\end{footnotesize}


You can then read the value from the command line by adding the following to the main method of \code{Learn}:
\begin{footnotesize}
\begin{verbatim}
int gradient_ascent_training_iterations = 5;
if (CommandLineUtilities.hasArg("gradient_ascent_training_iterations"))
    gradient_ascent_training_iterations = 
         CommandLineUtilities.getOptionValueAsInt("gradient_ascent_training_iterations");
\end{verbatim}
\end{footnotesize}

You should not change the order of examples. You must iterate over examples exactly as they appear in the data file, i.e. as provided by the data loader.




\subsubsection{Initialization}

You should initialize all $\alpha$ values to 0.


\subsubsection{Numeric Stability}


For all numerical calculations involving floating point numbers, use the {\tt double} type and NOT the {\tt float} type to store values.
This will help in achieving numerical precision.

\subsection{Data Sets}
We are providing a new synthetic data set for this assignment called \code{Circle}.



\section{Analytical (CS 6362 only: 50 points)}

\paragraph{1) Curse-of-dimensionality (10 points)}
In this problem, we study why $K$-NN could fail in high dimensions by a very simple example. Consider a sphere of radius $r$ in $d$-dimensions together with a concentric hypercube of side $2r$. The sphere touches the hypercube at the center of each of its sides.

\begin{enumerate}[(a)]
\item $V_c$ is the volume of the cube and $V_s$ is the volume of the sphere, where the volume of a $d$-dimensional sphere with radius $r$ is given as 
\begin{eqnarray}
V_s = \frac{r^d\pi^{d/2}}{\Gamma(d/2+1)}, \nonumber
\end{eqnarray}
where $\Gamma(z)=\int_0^{\infty}t^{z-1}e^{-t}\textrm{d}t$. Please show that: 
\begin{eqnarray}
\lim_{d\rightarrow\infty}\frac{V_s}{V_c} = 0
\label{CoD}
\end{eqnarray}

Note that this relies on algebra and will not require any complex calculus. You may find the following limit useful:
\begin{eqnarray}
\lim_{z\rightarrow\infty}\frac{\Gamma(z+1)}{\sqrt{2\pi z}e^{-z}z^z}=1 \nonumber
\end{eqnarray}

\item What is the connection between \eqref{CoD} and the curse of dimensionality?
\end{enumerate}

\paragraph{2) Overfitting (10 points)}
SVMs using nonlinear kernels usually have two tuning parameters (regularization parameter $C$ and kernel parameter $\gamma$), which are usually determined by cross validation. Suppose we use cross validation to determine the non-linear kernel parameter and slack variable for an SVM. We find that classifiers using parameters $(c_1,\gamma_1)$ and $(c_2,\gamma_2)$ achieve the same cross validation error, but $(c_1,\gamma_1)$ leads to fewer support vectors than $(c_2,\gamma_2)$. Explain which set of parameters should we choose for the final model?

\paragraph{3) Hinge Loss (10 points)}

Linear SVMs can be formulated in an unconstrained optimization problem
\begin{align}\label{SVM}
\min_{w,b}\sum_{i=1}^n H(y_i(w^Tx_i)) + \lambda\|w\|_2^2,
\end{align}
where $\lambda$ is the regularization parameter and $H(a) = \max(1-a,0)$ is the well known hinge loss function. The hinge loss function can be viewed as a convex surrogate of the 0/1 loss function $I(a \leq 0)$.
\begin{enumerate}[(a)]
\item Prove that $H(a)$ is a convex function of $a$.
\item If $H'(a) = \max(0.5-a,0)$, please show that there exists $\lambda'$ such that \eqref{SVMa} is equivalent to \eqref{SVM}
\begin{align}\label{SVMa}
\min_{w,b}\sum_{i=1}^n H'(y_i(w^Tx_i)) + \lambda'\|w\|_2^2.
\end{align}
\end{enumerate}

\paragraph{4) Kernel Trick (10 points)}
The kernel trick extends SVMs to handle nonlinearly separable data sets. However, an improper use of a kernel function can cause serious over-fitting. Consider the following kernels.
\begin{enumerate}[(a)]
\item Polynomial kernel: $K(x, x') = (1 + (x x'^T))^d$, where  $d\in\mathbb{N}$. Does increasing $d$ make over-fitting more or less likely?
\item Gaussian kernel: $K(x, x') = \exp(-|| x-x'||^2 / 2 \sigma^2)$, where $\sigma>0$. Does increasing $\sigma$ make over-fitting more or less likely?
\end{enumerate}
We say $K$ is a kernel function, if there exists some transformation $\phi:\mathbb{R}^m\rightarrow \mathbb{R}^{m'}$ such that $K(x_i,x_{i'}) = \left<\phi(x_i),\phi(x_{i'})\right>$.
\begin{enumerate}[(c)]
\item Let $K_1$ and $K_2$ be two kernel functions. Prove that $K(x_i,x_{i'}) = K_1(x_i,x_{i'}) + K_2(x_i,x_{i'})$ is also a kernel function.
\end{enumerate}

\paragraph{5) Prediction using Kernel (10 points)} 
One of the differences between linear SVMs and kernel SVMs concerns computational complexity at prediction time.
\begin{enumerate}[(a)]
\item What is the computational complexity of prediction of a linear SVM in terms of the examples $n$ and features $m$?
\item What is the computational complexity of prediction of a non-linear SVM in terms of the examples $n$, features $m$ and support vectors $s$?
\end{enumerate}


\section{What to Submit}
In each assignment you will submit two things.
\begin{enumerate}
\item {\bf Code:} Your code as a zip file named {\tt hw3code.zip}. {\bf You must submit source code (.java files)}. We will run your code using the exact command lines described above, so make sure it works ahead of time. Remember to submit all of the source code, including what we have provided to you.
\item {\bf Writeup:} Your writeup as a {\bf PDF file} (compiled from latex) containing answers to the analytical questions asked in the assignment. Make sure to include your name in the writeup PDF and use the provided latex template for your answers.
\end{enumerate}
Make sure you name each of the files exactly as specified ({\tt
  hw3code.zip} and {\tt hw3solutions.pdf}).


\section{Appendix: Deriving the Gradient for Kernel Logistic Regression}

In this section, we show how the gradient $\nabla \ell(Y,X,\va)$ is derived.
We begin by writing the conditional likelihood:
\begin{equation}
P(Y \mid \va, X) = \prod_{i=1}^N{p(\yi \mid \va, \vxi)}
\end{equation}

Since $\yi \in \{ 0,1 \}$, we can write the conditional probability inside the product as:
\begin{equation}
P(Y \mid \va, X) = \prod_{i=1}^N{ p(\yi=1 \mid \va, \vxi)^{\yi} \times (1-p(\yi=0 \mid \va, \vxi))^{1-\yi}}
\end{equation}
Note that one of these terms in the product will have an exponent of 0, and will evaluate to 1.\\

For ease of math and computation, we will take the log:
\begin{equation}
\ell(Y,X,\va) = \log P(Y \mid \va, X) = \sum_{i=1}^N{ \yi \log(p_{\yi=1}(\va, \vxi)) + (1-\yi)\log(p_{\yi=0}(\va, \vxi))}
\end{equation}

Plug in our logistic function $g$ for the probability that $y$ is 1:
\begin{equation}
\ell(Y,X,\va) = \sum_{i=1}^N{ y_i \log( g(\textstyle\sum_{j=1}^N \alpha_j K(\vx_j, \vx_i) ) ) + (1-y_i) \log( 1-g(\textstyle\sum_{j=1}^N \alpha_j K(\vx_j, \vx_i) ) )}
\end{equation}

To keep the notation clean, we will use the shorthand:
\begin{equation}
\kappa(\vxi) = \sum_{j=1}^N \alpha_j K(\vx_j, \vx_i)
\end{equation}

With this notation, we have:
\begin{equation}
\ell(Y,X,\va) = \sum_{i=1}^N{ y_i \log( g(\kappa(\vxi)  ) ) + (1-y_i) \log( 1-g(\kappa(\vxi)  ) )}
\end{equation}

Recall that the link function, $g$, is the logistic function. It has the nice property $1 - g(z) = g(-z)$.
\begin{equation}
\ell(Y,X,\va) = \sum_{i=1}^N{ y_i\ \log( g(\kappa(\vxi) ) ) + (1-y_i)\ \log( g(-\kappa(\vxi) ) )}
\end{equation}

We can now use the chain rule to take the gradient with respect to $\va$:
\begin{equation}
\nabla \ell(Y,X,\va) = \sum_{i=1}^N 
	y_i\ \frac{1}{g\kappa(\vxi) )} \nabla g(\kappa(\vxi) ) 
	 + (1-y_i)\ \frac{1}{g(-\kappa(\vxi))} \nabla g(-\kappa(\vxi)) 
\end{equation}

Since $\frac{\partial}{\partial z}g(z) = g(z)(1-g(z))$:
\begin{eqnarray}
\nabla \ell(Y,X,\va) &=& \sum_{i=1}^N 
	 y_i\ \frac{1}{g(\kappa(\vxi) )} g(\kappa(\vxi)) (1-g(\kappa(\vxi) )) \nabla (\kappa(\vxi)) \\
& & 	+ (1-y_i)\ \frac{1}{g(-\kappa(\vxi) )} g(-\kappa(\vxi) ) (1-g(-\kappa(\vxi) )) \nabla (-\kappa(\vxi) )
\end{eqnarray}

Simplify again using $1-g(z) = g(-z)$ and cancel terms
\begin{equation}
\nabla \ell(Y,X,\va) = \sum_{i=1}^N{
	y_i g(-\kappa(\vxi) ) \nabla(\kappa(\vxi) )
	+ (1-y_i) g(\kappa(\vxi) ) \nabla (-\kappa(\vxi) )
}
\end{equation}

You can now get the partial derivatives (components of the gradient) out of this gradient function by:

\begin{equation}
\pder[\ell]{\alpha_k} = \sum_{i=1}^{N} y_i g\left(-\textstyle \sum_{j=1}^{N} \alpha_j K(x_j, x_i)\right) \left(K(x_i, x_k)\right) 
+ (1 - y_i) g\left(\textstyle \sum_{j=1}^{N} \alpha_j K(x_j, x_i)\right) \left(-K(x_i, x_k)\right) 
\end{equation}

\noindent because

\begin{equation}
\pder{\alpha_k}\kappa(\vx_i)  = \pder{\alpha_k}\left[ \textstyle\sum_{j=1}^N \alpha_j K(\vx_j, \vx_i) \right] = K(\vx_k, \vx_i).
\end{equation}

\end{document}

