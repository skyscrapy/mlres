\documentclass[11pt]{article}

\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{url}
\usepackage{wrapfig}
\usepackage{hyperref} 
\usepackage{color}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{enumerate}
\usepackage{amsmath,bm}
\oddsidemargin 0mm
\evensidemargin 5mm
\topmargin -20mm
\textheight 240mm
\textwidth 160mm

\newcommand{\alphai}{\alpha_i}
\newcommand{\va}{{\bf \alpha}}
\newcommand{\vw}{{\bf w}}
\newcommand{\vx}{{\bf x}}
\newcommand{\vxi}{{\bf x}_i}
\newcommand{\vxij}{{\bf x}_{ij}}
\newcommand{\vt}{{\bf t}}
\newcommand{\X}{{\mathbf X}}
\newcommand{\Y}{{\bf Y}}
\newcommand{\yh}{\hat{y}}
\newcommand{\yi}{y_i}
\newcommand{\pder}[2][]{\frac{\partial#1}{\partial#2}}

\def\P{\mathbb{P}}
\def\E{\mathbb{E}}
\def\R{\mathbb{R}}
\newcommand{\msigma}{{\bf \Sigma}}
\newcommand{\vmu}{{\bf \mu}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\el}{\mathcal{L}}
\newcommand{\code}[1]{{\footnotesize \tt #1}}
\renewcommand{\baselinestretch}{1.2}
\pagestyle{myheadings} 
\markboth{Homework 2}{Spring 2016 CS 4269/6362 Machine Learning: Homework 2}


\title{CS 4269/6362 Machine Learning: Homework 2\\Linear Models for Supervised Learning\\
\Large{Due: 1/28/2016}\\
150 Points Total \hspace{1cm} Version 1.0}

\author{}
\date{}

\begin{document}
\large
\maketitle
\thispagestyle{headings}

\vspace{-.5in}
\section{Programming (100 points; 150 points for CS 4269)}
In this assignment you will write three learning algorithms: basic linear
regression, a Naive Bayes classifier, and a Perceptron classifier. Both classifiers need only support binary prediction.

\subsection{Linear Regression}

Recall from the lecture that linear regression model takes the form
\[
f(x) = w^T x,
\]
where $x$ is a feature vector, and we assume that $x_1 = 1$ (this unit
feature can always be appended to the given feature vector; indeed,
you will append this unit feature in your implementation of the linear
regression).
Given a data set of examples where $\mathbf{X}$ is a matrix with rows
as feature vectors corresponding to different data points and
$y$ is a corresponding observed output vector, we can train
the weight vector $w$ as follows:
\[
w = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T y.
\]
Your task is to implement linear regression training and prediction.

While you will be allowed to use the Apache math commons package for
this assignment, {\bf you can only use the libraries in
  org.apache.commons.math3.linear} (i.e., the linear algebra libraries).

\subsubsection{Regression Data}
For this portion of the assignment you will use the provided regression dataset.

\paragraph{{\bf NASDAQ}} A popular example of regression is to predict stock prices. The goal of this dataset is to predict the price of the NASDAQ 500 index each day given the index value from the three previous days. The three attributes correspond to the price the day before (1), two days prior (2) and three days prior (3). We leave it to you to determine what reasonable accuracy for this data set should be.

Note that the script \code{compute\_accuracy.py} does not support regression. 
Instead, we are providing a new script
\code{compute\_regression\_accuracy.py} which computes the mean
squared error. Note that the output says Accuracy, but it's not. Take
a look at the Python code if you are curious.

\subsubsection{Running Code}
We will evaluate your algorithm by running the following commands for
{\bf easy}, {\bf hard}, and {\bf nasdaq} data sets:
\begin{footnotesize}
\begin{verbatim}
java cs362.Learn -mode train -algorithm linear_regression -model_file easy.linear_regression.model \
                     -data easy.train - task regression
\end{verbatim}
\end{footnotesize}
To run the trained model on development data:
\begin{footnotesize}
\begin{verbatim}
java cs362.Learn -mode test -model_file easy.linear_regression.model -data easy.dev \
                     -predictions_file easy.dev.linear_regression.predictions -task regression
\end{verbatim}
\end{footnotesize}

\subsection{Naive Bayes}
Implement a Naive Bayes classifier presented in class. A few details will help your
implementation:

%\paragraph{Continuous features}
%The Naive Bayes equations naturally handle binary features. 
%For continuous features, select the mean of the feature {\em in the entire training set}. If a feature's value is \emph{less than or equal to} the mean, pretend as if feature A is observed. If the feature's value is \emph{greater than} the mean, pretend as if feature B is observed. Therefore, each continuous feature will be split into two features (A and B). Remember, for computing the mean value of a feature, examples in which the feature does not appear have value $0$ for that feature. Remember, you can determine binary features by examining features in the data that only appear with value $1$ or $0$.


\paragraph{New Features} There may be new features encountered at test time that are not present in the training data. For simplicity, skip these features when creating predictions.\\


Note that there is no bias term in this version and you should
\emph{not} include one in your solution. Implement this algorithm as the \code{NaiveBayesClassifier} class. Your Naive Bayes classifier should be selected by passing the string {\tt naive\_bayes} as the argument for {\tt algorithm.} 

\subsubsection{Smoothing}

Naive Bayes is prone to overfitting because test instances with rare or unobserved features could be assigned unfairly low probabilities for some or all labels. 
Therefore, you will {\em smooth} your probability estimates using a technique commonly called add-$\lambda$ smoothing. Smoothing these distributions is simple: increase each feature/label count by $\lambda$, for some real-valued $\lambda \ge 0$. For this reason, the $\lambda$ terms are often referred to as {\em pseudocounts}, because you are pretending to count each feature/label more than actually observed. This makes it so that unseen features will still have some probability mass at test time.
When $\lambda=1$, this is called Laplacian smoothing (or simply +1 smoothing). 

Remember to adjust the normalization constants accordingly so that your probabilities still sum to 1. You should smooth both $p(x|y)$ and $p(y)$.



You {\em must} add a command line argument to allow $\lambda$ to be adjusted via the command line. The default value should be 1.0.
Add this command line option by adding the following code to the \code{createCommandLineOptions} method of \code{Learn}.
\begin{footnotesize}
\begin{verbatim}
registerOption("lambda", "double", true, "The level of smoothing for Naive Bayes.");
\end{verbatim}
\end{footnotesize}

You can then read the value from the command line by adding the following to the main method of \code{Learn}:
\begin{footnotesize}
\begin{verbatim}
double lambda = 1.0;
if (CommandLineUtilities.hasArg("lambda"))
    lambda = CommandLineUtilities.getOptionValueAsFloat("lambda");
\end{verbatim}
\end{footnotesize}

\subsubsection{Handling Non-Binary Features}

In your code you will treat all features as binary.
For all features which are not binary (e.g., continuous features),
assume that the feature value is $0$ when it is $<0.5$ and $1$ otherwise.


\subsubsection{Making Predictions}

When making predictions using the learned Naive Bayes model, you should use the following prediction rule: choose the label $y$ which satisfies

\begin{displaymath}
\arg\max_{y \in \{0,1\}} p(y) \prod_{j \in \vx} p(x_j | y)
\end{displaymath}

\noindent where the product is over the features contained in the instance $\vx$. In the case of a tie, let $y=1$.

\paragraph{Handling Underflow} Notice that the prediction rule involves a product of multiple terms which may have values close to zero. If you repeatedly multiply small values together, this product will quickly shrink, and the floating point is likely to underflow. The most common way to deal with this issue is to convert the product into a summation by taking the log of the probabilities. Recall that $\arg\max_{x} f(x) = \arg\max_{x} \log f(x)$, \noindent because $\log(x)$ monotonically increases with $x$. As you learned in class, this property is also exploited in maximum likelihood estimation, where the goal is to maximize the log-likelihood, because it is analytically easier to work with log probabilities. The prediction rule you should use is thus:

\begin{displaymath}
\arg\max_{y \in \{0,1\}} \log(p(y)) +  \sum_{j \in \vx} \log(p(x_j | y))
\end{displaymath}


\subsubsection{Running Code}
We will evaluate your algorithm by running the following commands (we'll additionally specify the required command line parameter):
\begin{footnotesize}
\begin{verbatim}
java cs362.Learn -mode train -algorithm naive_bayes -model_file bio.naive_bayes.model \
                     -data bio.train -task classification
\end{verbatim}
\end{footnotesize}
To run the trained model on development data:
\begin{footnotesize}
\begin{verbatim}
java cs362.Learn -mode test -model_file bio.naive_bayes.model -data bio.dev \
                     -predictions_file bio.dev.naive_bayes.predictions -task classification
\end{verbatim}
\end{footnotesize}

\subsubsection{Classification Data}

For this portion of the assignment you will use the bio, easy, and hard data sets from
homework 1.

\subsection{Perceptron Algorithm}

Perceptron is a mistake-driven online learning algorithms for linear
classifiers. It takes as input a vector of real-valued inputs $\vx$
and make a prediction $\yh \in \{-1,+1\}$ (for this assignment we
consider only binary labels). Predictions are made using a linear
classifier known as a linear threshold unit (LTU): $\yh =
\textrm{sign}((\vw \cdot \vx) - \beta)$, with a scalar threshold
$\beta$. The term $\vw \cdot \vx$ is the dot product of $\vw$ and
$\vx$ computed as $\sum_i x_i  w_i$. An LTU says that if this dot
product is greater than the threshold $\beta$ then the prediction is
$+1$; if the dot product is less than the threshold $\beta$, the
prediction is $-1$.
Below, we will use the threshold $\beta = 0$.
{\bf BEWARE: because the data sets we provide code output values as 0/1 rather
than -1/+1, you will need to convert the observed labels to the -1/+1
coding scheme, and vice versa as you train and evaluate the model.}

Updates to $\vw$ are made only when a prediction is incorrect: $\yh \ne y$. The new weight vector $\vw'$ is a function of the current weight vector $\vw$ and example $(\vx, y)$. The weight vector is updated so as to improve the prediction on the current example. Note that this algorithm naturally handles both continuous and binary features, so no special processing is needed.

\subsubsection{Basic Algorithm}
\label{sec:algorithm}

The basic structure of the linear threshold based algorithms is:
\begin{enumerate}
\item Initialize $\vw = {\bf 0}$, set learning rate $\eta$ and number of iterations $I$
\item For each training iteration $k = 1 \ldots I$:
	\begin{enumerate}
	\item For each example $i=1\ldots N$:
	\begin{enumerate}
		\item Receive an example $\vxi$
		\item  \label{prediction_rule} Predict the label $\hat{\yi}  =\left\{
		     \begin{array}{lr}
		       1 ~~  \textrm{if} ~~ \vw \cdot \vxi \ge 0  \\
		       -1 ~~  \textrm{if} ~~ \vw \cdot \vxi < 0  
		     \end{array}
		   \right.$
		\item \label{update_step} If  $\hat{\yi} \ne \yi$,
                  make an update to $\vw$. The update $\vw' = \vw + \eta \yi \vxi$.
		\end{enumerate}
	\end{enumerate}
\end{enumerate}

Implement this algorithm as the \code{PerceptronClassifier}
class. Your Perceptron predictor will be selected by passing the
string \code{perceptron} as the argument for the algorithm parameter. (Note that there is no bias term in this version and you should \emph{not} include one in your solution.)

\subsubsection{Learning Rate}
Perceptron uses a learning rate $\eta$, where  $0 < \eta \leq 1$.  Your default value for $\eta$ should be $1$. You \emph{must} add a command line argument to allow this value to be adjusted via the command line. 

Add this command line option by adding the following code to the \code{createCommandLineOptions} method of \code{Learn}.
\begin{footnotesize}
\begin{verbatim}
registerOption("online_learning_rate", "double", true, "The LTU learning rate.");
\end{verbatim}
\end{footnotesize}

Be sure to add the option name exactly as it appears above. A common mistake is to change underscores to dashes.

You can then read the value from the command line by adding the following to the main method of \code{Learn}:
\begin{footnotesize}
\begin{verbatim}
double online_learning_rate = 1.0;
if (CommandLineUtilities.hasArg("online_learning_rate"))
    online_learning_rate = CommandLineUtilities.getOptionValueAsFloat("online_learning_rate");
\end{verbatim}
\end{footnotesize}

\subsubsection{Number of training iterations}
Since we will be running these online methods in the batch setting, you can iterate multiple times over the data. This can improve performance by increasing the number of updates each algorithm makes. We will define the number of times each algorithm iterates over the data by the parameter \code{online\_training\_iterations}. You \emph{must} define a new command line option for this parameter. Use a default value of $1$ for this parameter.

You can add this option by adding the following code to the \code{createCommandLineOptions} method of \code{Learn}.
\begin{footnotesize}
\begin{verbatim}
registerOption("online_training_iterations", "int", true, "The number of training iterations for LTU.");
\end{verbatim}
\end{footnotesize}


You can then read the value from the command line by adding the following to the main method of \code{Learn}:
\begin{footnotesize}
\begin{verbatim}
int online_training_iterations = 1;
if (CommandLineUtilities.hasArg("online_training_iterations"))
    online_training_iterations = CommandLineUtilities.getOptionValueAsInt("online_training_iterations");
\end{verbatim}
\end{footnotesize}

\subsubsection{Running Code}
We will evaluate your algorithm by running the following commands
(we'll additionally specify the required command line parameters):
\begin{footnotesize}
\begin{verbatim}
java cs362.Learn -mode train -algorithm perceptron -model_file bio.perceptron.model \
                     -data bio.train -task classification
\end{verbatim}
\end{footnotesize}
To run the trained model on development data:
\begin{footnotesize}
\begin{verbatim}
java cs362.Learn -mode test -model_file bio.perceptron.model -data bio.dev \
                     -predictions_file bio.dev.perceptron.predictions -task classification
\end{verbatim}
\end{footnotesize}

\subsubsection{Classification Data}

For this portion of the assignment you will use the bio, easy, and hard data sets from
homework 1.

\subsection{Implementation Details}

For all numerical calculations involving floating point numbers, use the {\tt double} type and NOT the {\tt float} type to store values.
This will help in achieving numerical precision.



\section{CS 6362 ONLY: Analytical (50 points)}

{\bf NOTE: PLEASE USE THE PROVIDED answers\_form.tex TEMPLATE FOR YOUR
ANSWSERS!  SUBMIT THE COMPILED PDF.}

\paragraph{1) Basic Concepts (20 points)}
Generalized linear models (GLMs), especially logistic regression are heavily used by banks, credit card companies and insurance companies. Actually, when you apply for a credit card, banks may put your information into a logistic regression model to decide whether you are eligible.
\begin{enumerate}[(a)]
\item The GLMs are closely related to the exponential distribution family, which has the probability density/mass function $f(X=x; \theta)$ in the form
\begin{align}
f(X=x; \theta) = h(x)e^{\eta(\theta)\cdot T(x)-A(\theta)},
\end{align}
where $h,\eta,T,A$ are some known functions. Given 
\begin{description}
\item (1) the probability mass function of the binomial distribution (assume that $n$ is given)
\begin{align}
f(X=x; p) = {{n}\choose{x}}p^x(1-p)^{n-x},~x\in\{0,1,...,n\};
\end{align}
\item (2) the probability mass function of the Poisson distribution
\begin{align}
f(X=x; \lambda) = \frac{\lambda^xe^{-\lambda}}{x!};
\end{align}
\item (3) the probability density function of the Gaussian distribution (assume that $\sigma$ is given)
\begin{align}
f(X=x; \mu) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}
\end{align}
\end{description}
Please show that these three distributions belong to the exponential family.

\item  When the
  probability of an event, such as a coin landing heads or classifier
  output being $y = +1$, is $p$, the odds ratio is
  $p/(1-p)$. We see that $\eta(\theta)$ for the binomial distribution is
  exactly the log-odds-ratio in the logistic regression. That is why that the logistic regression is also called binomial regression. Therefore given $\eta(\theta)$ for the Poisson distribution, and $n$ observations $(x_i, y_i)_{i=1}^n$, what is the log-likelihood function for the Poisson regression?

\item The GLMs often contain some transformation, which is non-linear such as the log-odds-ratio transformation in the logistic regression. Why do we still call them ``linear"?

\end{enumerate}

\paragraph{2) Missing Values (10 points)} 

One advantage of naive Bayes over the logistic regression is that it can handle missing values in the data (i.e. not knowing the value of a specific feature). Given a trained naive Bayes classifier, show how to calculate $\mathbb{P}(Y, X_1, X_2, ..., X_{d-1})$ where $X_d$ is unknown. Assume $Y \in \{0,1\}$ and $X_j \in \{1,2,...,K\}$ where $j=1,...,d$. [Hint: The conditional probability in naive Bayes can be completely factorized.]

\paragraph{3) Add-$\lambda$ Smoothing (10 points)} 

When training a naive Bayes classifier, it is possible to assign zero values to some conditional probabilities. Given $n$ observations, $(x_i,y_i)_{i=1}^n$, where $x_i = (x_{i1},...,x_{id})$, we then have
\begin{align}\label{original-one}
\hat{\mathbb{P}}(X_j=x|Y=y) = \frac{\sum_{i=1}^nI(x_{ij}=x,y_i=y)}{\sum_{i=1}^nI(y_i=y)}.
\end{align}
where $I(x_{ij}=x,y_i=y)$ is the indicator function that is $1$ iff the $j$th feature in the $i$th example has value $x$ and the label for the $i$th example is $y$.

For example, if $\sum_{i=1}^nI(x_{ij}=1,y_i=0)=0$, then $\hat{\mathbb{P}}(X_j=x|Y=y) = 0$. While some people prefer to use a different estimator
\begin{align}\label{add-one}
\hat{\mathbb{P}}(X_j=x|Y=y) = \frac{\lambda+\sum_{i=1}^nI(x_{ij}=x,y_i=y)}{K\lambda+\sum_{i=1}^nI(y_i=y)}.
\end{align}
\eqref{add-one} is the so-called ``add-$\lambda$ smoothing" procedure. What role does $\lambda$ play in terms of the bias variance tradeoff? 

\paragraph{4) Stochastic Gradient Algorithm (10 points)}

The stochastic gradient algorithm is a very powerful optimization tool to solve many online learning problems. Instead of computing the gradient over the entire data set before making an update, the stochastic gradient algorithm computes the gradient over a single example, then updates the parameters. By passing over the entire data set in this fashion we can converge to the optimal parameters.

A single iteration of stochastic gradient considers a single example. As a result, we know that stochastic gradient converges more slowly than gradient descent. While it takes many more iterations, each iteration is much faster, both in terms of memory and computation.

Consider a linear regression problem with $n$ samples:
\begin{align}
\hat{\beta} = \arg\min_{w}\frac{1}{2}\sum_{i=1}^n(y_i - w^T x_i)^2.
\end{align}
In each iteration, instead of using only one example, we randomly choose $k$ out of $n$ samples and obtain $(x_{1'},y_{1'}),...,(x_{k'},y_{k'})$. 
\begin{enumerate}[(a)]
\item What is the computational complexity of computing the gradient at each iteration (using $k$ and $n$)?
\item What are the advantages/disadvantages of increasing $k$ in terms of computational complexity (using $k$ and $n$)? What is traded-off by increasing/decreasing $k$?
\item What is an example of an algorithm we learned in class that uses stochastic gradient? Explain.
\end{enumerate}


\section{What to Submit}
In each assignment you will submit two things on OAK.
\begin{enumerate}
\item {\bf Code:} Your code as a zip file named {\tt hw2code.zip}. {\bf You must submit source code (.java files)}. We will run your code using the exact command lines described above, so make sure it works ahead of time. Remember to submit all of the source code, including what we have provided to you.
\item {\bf Writeup:} Your writeup ({\tt hw2solution.pdf}) as a {\bf PDF file} (compiled from latex) containing answers to the analytical questions asked in the assignment. Make sure to include your name in the writeup PDF and use the provided latex template for your answers.
\end{enumerate}
Make sure you name each of the files exactly as specified ({\tt
  hw2code.zip} and {\tt hw2solution.pdf}).

\end{document}

